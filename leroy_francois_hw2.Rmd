--- 
title: |
  | Introduction to Machine Learning
  | (NPFL054)
author: "Fran√ßois Leroy, PhD student at CZU"
date: "`r Sys.Date()`"
output: pdf_document
documentclass: report
classoption: oneside
fontsize: 12pt
linestretch: 1.5
geometry:
- left = 2cm
- right = 2cm
- top = 2cm
- bottom = 2cm
colorlinks: yes
link-citations: yes
github-repo: FrsLry/HW1_ML_CUNI_leroy
subtitle: Homework 2
---

```{r, echo = F}
knitr::opts_chunk$set(warning = F,
                      message = F)
```

# Set up the project {-}

```{r}
rm(list = ls())
library(ISLR) # for the data
library(tidyverse) # convenient
library(rpart) # for decision trees
library(randomForest) # for ensemble learning
library(glmnet) # for regularized logistic regression
library(ROCR) # for ROC curves
```

```{r}
## Reproduce the result
set.seed(123)
## Create the splitting vector
split <- sample(nrow(Caravan), 1000)
## Create the test dataset
d_test <- Caravan[split,]
## Create the training dataset
d_train <- Caravan[-split,]
```


# Task 1 - Data analysis

* **First, check the distribution of the target attribute. What would be your precision if you select 100 examples by chance?**

```{r}
round(table(Caravan$Purchase), 2)
```

```{r, echo=F}
Caravan %>% 
  ggplot()+
  geom_bar(aes(x = Purchase, y = ..prop.., group = 1), stat = "count")+
  ylab("Proportion")+
  theme_bw()
```

```{r}
plot(dbinom(1:20, size = 100, prob = .06))
```

We can see that there is `r paste0(round(table(Caravan$Purchase)/nrow(Caravan), 2)[1]*100, "%")` of customers who didn't purchase  the insurance and that `r paste0(round(table(Caravan$Purchase)/nrow(Caravan), 2)[2]*100, "%")` who did. As the precision is the number of examples classified as *Yes* when the value is actually *Yes*, by chance, the precision should be `r round(table(Caravan$Purchase)/nrow(Caravan), 2)[2]`.

* **1.a. Focus on the customer type MOSHOOFD: create a table with the number of customers that belong to each of 10 L2 groups and the percentage of customers that purchased a caravan insurance policy in each group. Comment the figures in the table. Then do the same for the customer subtype MOSTYPE (41 subgroups defined in L1).** 


\underline{MOSHOOFD type:}

```{r}
Caravan %>% 
  count(MOSHOOFD, Purchase) %>% 
  group_by(MOSHOOFD) %>% 
  summarise(size = sum(n),
            purchase_prop = round(n[Purchase == "Yes"]/sum(n), 2)) %>% 
  rename(group = MOSHOOFD) %>% 
  kableExtra::kable()
```

From this first table, we can see that the customers that are more prone to purchase an insurance (13% of them) are the one belonging to the group 2, *i.e.* the *driven growers*. On the other hand, the customers belonging to the class 6 and 10, respectively the *cruising seniors* and the *farmers*, are less likely to subscribe to the insurance (only 2% in each group).

\underline{MOSHOOFD type:}

```{r}
table <- 
Caravan %>% 
  count(MOSTYPE, Purchase) %>% 
  group_by(MOSTYPE) %>% 
  summarise(size = sum(n),
            purchase_prop = round(n[Purchase == "Yes"]/sum(n), 2)) %>% 
  rename(group = MOSTYPE) %>% 
  arrange(desc(purchase_prop))
## Display in 2 columns
kableExtra::kable(list(table[1:(nrow(table)/2),], 
                       table[((nrow(table)/2)+1):nrow(table),])) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")
```

The two groups more prone to buy an insurance are the group 8 and 12, which correspond respectively to *middle class families* and *affluent young families*. Thus, we can say that families are potential good targets to sell insurances. We can see that the class 25, 26, 27 and 29 all have a low proportion of individuals buying a insurance. They are all related to old people (*i.e.*, *Young seniors in the city*, *Own home elderly*, *Seniors in apartments*, *Porchless seniors: no front yard*). Thus, old people are not a good target to sell insurances. 

**1.b. Analyze the relationship between features MOSHOOFD and MOSTYPE.**

```{r}
Caravan %>% 
  ggplot(aes(y = MOSTYPE, x = MOSHOOFD))+
  geom_point()+
  geom_smooth(method = "lm")+
  theme_bw()
```

We can clearly see a relationship between these two features which are MOSHOOFD = *Customer main type* and MOSTYPE = *Customer Subtype*. This is expected because MOSTYPE is just a more precise social position. For instance, we can see that when $MOSHOOFD = 10$, $MOSTYPE = 40 | 41$. We can see that $MOSHOOFD = 10$ correspond to *Farmers* and that $MOSTYPE = 40 | 41$ are two subclasses of farmers: *Large family farms* and *Mixed rurals*, respectively. 

# Task 2 - Model fitting, optimization, and selection

```{r}
## Function to randomly extract the test dataset in d_train 
## using always the same number of positive and negative 
## values of Purchase
prepare_cv_folds <-  function(k){
  # Create the subsets data containing Purchase == Yes 
  # in one hand and Purchase == No in an other hand
  pos_data <- d_train[d_train$Purchase == "Yes",]
  neg_data <- d_train[d_train$Purchase == "No",]
  ## Compute the size of each fold
  fold.size.pos <- nrow(pos_data)%/%k
  fold.size.neg <- nrow(neg_data)%/%k
  ## Randomly rearrange the indexes
  set.seed(12); s_pos <- sample(nrow(pos_data))
  set.seed(12); s_neg <- sample(nrow(neg_data))
  ## create the list that will contain the test folds
  f.idx <-  list()
  ## For each fold, extract the dataset that will be used as test
  for(i in 1:k){
      f.idx[[i]] <- 
        rbind(pos_data[s_pos[(1 + (i-1)*fold.size.pos):(i*fold.size.pos)],],
              neg_data[s_neg[(1 + (i-1)*fold.size.neg):(i*fold.size.neg)],])
  }
  return(f.idx)
}
## Use the function to create the 10 test datasets
split_data <- prepare_cv_folds(10)
```

## Decision tree

```{r, echo=F, eval=F}
set.seed(123)
## Sequence of cp
seq <- seq(1e-05, 0.001, length.out = 10)
## table that will contain the auc for each fold
auc_table <- tibble(auc = as.numeric())
## table summarizing the cv for each cp
dt_auc <- tibble(cp = as.numeric(),
                 mean_auc = as.numeric(),
                 sd = as.numeric(),
                 low_CI = as.numeric(),
                 high_CI  = as.numeric())
## Loop over each cp
for(j in 1:length(seq)){
  ## Loop over each fold
  for(i in 1:10){
    ## Get the indexes of the test datasets
    test_index <- rownames(split_data[[i]])
    cv_train_data <-
      d_train %>% 
      filter(!rownames(.) %in% test_index) %>% 
      mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
      mutate(Purchase = as.factor(Purchase))
    cv_test_data <-
      split_data[[i]] %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
    ## Learn decision tree without the test dataset 
    DT <- rpart(Purchase ~ ., cp = seq[j],
                data = cv_train_data)
    pred <- predict(DT, (cv_test_data %>% select(-Purchase)), type = "prob")[,2]
    p <- prediction(pred, cv_test_data$Purchase)
    auc <- performance(p, measure="auc", fpr.stop = .2)
    auc <- auc@y.values[[1]]
    auc_table[i, 1] <- auc
    # plot(performance(p, measure = 'tpr', x.measure = 'fpr'))
  }
  ## Fill the table
  dt_auc[j, 1] <- seq[j]
  dt_auc[j, 2] <- mean(auc_table$auc)
  dt_auc[j, 3] <- sd(auc_table$auc)
  dt_auc[j, 4] <- t.test(auc_table$auc)$"conf.int"[1]
  dt_auc[j, 5] <- t.test(auc_table$auc)$"conf.int"[2]
}
## Plot the table
dt_auc %>% 
  ggplot(aes(x = cp, y = mean_auc))+
  geom_point()+
  geom_point(aes(y = high_CI), color = "red")+
  geom_point(aes(y = low_CI), color = "red")+
  geom_pointrange(aes(ymin = mean_auc - sd, ymax = mean_auc + sd))+
  # ylim(0, .05)+
  theme_bw() 
ggsave("data/cv_dt.png")
```



```{r, echo=F}
knitr::include_graphics("data/cv_dt.png")
```


The graphique above shows the mean AUC as a function of different values of cp. The black lines represent the standard deviation and the red dots the Confidence Intervals. As we can see on this plot, reducing the complexity parameter below $cp = 0.001$ doesn't change the mean $AUC_{0.2}$. This means that $cp = 0.001$ is already sufficiently low. As a low cp means a more complex model, we are looking for the highest value of cp maximizing the mean AUC. Thus, we can select $cp = 0.001$ to learn the decision tree. 

However, as we can see, the mean AUC is always equal to 0.027, which is rather disappointing.


## Random Forest


```{r, echo=F, eval=F}
set.seed(123)
## Sequence of mtry
seq_mtry <- round(seq(2, 89, length.out = 6))
## Sequence of trees
seq_tree <- round(seq(10, 150, length.out = 10))
## table that will contain the auc for each fold
auc_table <- tibble(auc = as.numeric())
## table summarizing the cv for each cp
dt_auc <- tibble(ntree = as.numeric(),
                 mean_auc = as.numeric(),
                 sd = as.numeric(),
                 low_CI = as.numeric(),
                 high_CI  = as.numeric(),
                 mtry = as.numeric())
## Loop over each mtry
for(m in 1:length(seq_mtry)){
  ## Loop over each ntree
  for(j in 1:length(seq_tree)){
    ## Loop over each fold
    for(i in 1:10){
      ## Get the indexes of the test datasets
      test_index <- rownames(split_data[[i]])
      cv_train_data <-
        d_train %>% 
        filter(!rownames(.) %in% test_index) %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
      cv_test_data <-
        split_data[[i]] %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
      ## Learn decision tree without the test dataset 
      DT <- randomForest(Purchase ~ ., ntree = seq_tree[j], mtry = seq_mtry[m],
                  data = cv_train_data)
      pred <- predict(DT, (cv_test_data %>% select(-Purchase)), type = "prob")[,2]
      p <- prediction(pred, cv_test_data$Purchase)
      auc <- performance(p, measure="auc", fpr.stop = .2)
      auc <- auc@y.values[[1]]
      auc_table[i, 1] <- auc
      # plot(performance(p, measure = 'tpr', x.measure = 'fpr'))
    }
    ## Fill the table
    dt_auc[nrow(dt_auc)+1, 1] <- seq_tree[j]
    dt_auc[nrow(dt_auc), 2] <- mean(auc_table$auc)
    dt_auc[nrow(dt_auc), 3] <- sd(auc_table$auc)
    dt_auc[nrow(dt_auc), 4] <- ifelse(length(unique(auc_table$auc)) == 1,
                                      NA, t.test(auc_table$auc)$"conf.int"[1])
    dt_auc[nrow(dt_auc), 5] <- ifelse(length(unique(auc_table$auc)) == 1, 
                                      NA, t.test(auc_table$auc)$"conf.int"[2])  
    dt_auc[nrow(dt_auc), 6] <- seq_mtry[m]
  }
  
}
## Plot the table
dt_auc %>% 
  ggplot(aes(x = ntree, y = mean_auc))+
  geom_point()+
  geom_point(aes(y = high_CI), color = "red")+
  geom_point(aes(y = low_CI), color = "red")+
  facet_wrap(. ~ mtry, labeller = labeller(.cols = label_both))+
  geom_pointrange(aes(ymin = mean_auc - sd, ymax = mean_auc + sd))+
  # ylim(0, .05)+
  theme_bw()

ggsave("data/cv_rf.png")
```

```{r, echo=F}
knitr::include_graphics("data/cv_rf.png")
```


This plot shows the mean auc as a function of the number of trees. Each square correspond to a value of mtry indicated in the grey square. As we can see, the highest value of $AUC_{0.2}$ is for $mtry = 72$ and $ntree = 10$.

However, we must keep in mind that the Confidence Intervals always overlap, which means that the difference between the mean $AUC_{0.2}$ are not significant. 

## Regularized logistic regression

```{r, echo=F, eval=F}
# 5 different alpha values
alphas <- seq(0, 1, length.out = 5)
# 100 different lambda values
grid <- round(10^seq(4, -2, length = 10), 2)
## table that will contain the auc for each fold
auc_table <- tibble(auc = as.numeric())
## Table that will contain the summarized info
glmnet_table <- tibble(alpha = as.numeric(),
                       lambda = as.numeric(),
                       mean_auc = as.numeric(),
                       sd = as.numeric(),
                       low_CI = as.numeric(),
                       high_CI  = as.numeric())
## Loop over each alpha
for(iii in 1:length(alphas)){
  ## Loop over each lambda
  for(ii in 1:length(grid)){
    ## Loop over each fold
    for(i in 1:10){
      ## Get the indexes of the test datasets
      test_index <- rownames(split_data[[i]])
      cv_train_data <-
        d_train %>% 
        filter(!rownames(.) %in% test_index) %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
      cv_test_data <-
        split_data[[i]] %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
      ## Shape the train data for glmnet()
      x <- model.matrix(Purchase~., data = cv_train_data)
      y <- data.matrix(cv_train_data$Purchase)
      ## Shape the test data
      x.test <- model.matrix(Purchase~., data = cv_test_data)
      y.test <- data.matrix(cv_test_data$Purchase)
      ## Learn the model
      model <- glmnet(x, y, alpha = alphas[iii], family = "gaussian", lambda = grid[ii])
      ## Predict with the model
      pred <- predict(model, newx = x.test)
      p <- prediction(pred, cv_test_data$Purchase)
      ## Compute auc
      auc <- performance(p, measure="auc", fpr.stop = .2)
      auc <- auc@y.values[[1]]
      auc_table[i, 1] <- auc
      # plot(performance(p, measure = 'tpr', x.measure = 'fpr'))
    }
    glmnet_table[nrow(glmnet_table)+1, 1] <- alphas[iii]
    glmnet_table[nrow(glmnet_table), 2] <- grid[ii] 
    glmnet_table[nrow(glmnet_table), 3] <- mean(auc_table$auc)
    glmnet_table[nrow(glmnet_table), 4] <- sd(auc_table$auc)
    glmnet_table[nrow(glmnet_table), 5] <- ifelse(length(unique(auc_table$auc)) == 1,
                                                  NA, t.test(auc_table$auc)$"conf.int"[1])
    glmnet_table[nrow(glmnet_table), 6]<- ifelse(length(unique(auc_table$auc)) == 1,
                                                 NA, t.test(auc_table$auc)$"conf.int"[2])
  }
}
## Plot
glmnet_table %>% 
  ggplot(aes(x = as.factor(lambda), y = mean_auc))+
  geom_point()+
  geom_point(aes(y = high_CI), color = "red")+
  geom_point(aes(y = low_CI), color = "red")+
  facet_wrap(. ~ alpha, labeller = labeller(.cols = label_both))+
  geom_pointrange(aes(ymin = mean_auc - sd, ymax = mean_auc + sd))+
  # ylim(0, .05)+
  xlab("lambda")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
ggsave("data/cv_lr.png")
```

```{r, echo=F}
knitr::include_graphics("data/cv_lr.png")
```

## Evaluation on the test dataset

Thanks to the previous steps, we have chosen:

1. A decision tree with $cp = 0.00067$

2. A random forest with $ntree = 120$ and $mtry = 19$

3. A regularized logistic regression with $\lambda = 0.01$ and $\alpha = 0.25$



```{r, echo=F, fig.show="hold", out.width="150%"}
par(mfrow = c(2, 2))
final_comparison <- tibble(model =as.character(),
                           auc = as.numeric())
d_train <- 
  d_train %>% 
  mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
  mutate(Purchase = as.factor(Purchase))  

d_test <- 
  d_test %>% 
  mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
  mutate(Purchase = as.factor(Purchase))
  
## DT
opti_DT <- rpart(Purchase ~ ., data = d_train, cp = 0.00067)
pred <- predict(opti_DT, d_test %>% select(-Purchase), type = "prob")[,2]
p <- prediction(pred, d_test$Purchase)
final_comparison[1, 1] <- "Decision Tree"
final_comparison[1, 2] <- round(performance(p, measure="auc", fpr.stop = .2)@y.values[[1]], 3)
plot(performance(p, measure = 'tpr', x.measure = 'fpr'), main = "Decision Tree")


## RF
opti_RF <- randomForest(Purchase ~ ., data = d_train, ntree = 120, mtry = 19)
pred <- predict(opti_RF, d_test %>% select(-Purchase), type = "prob")[,2]
p <- prediction(pred, d_test$Purchase)
final_comparison[2, 1] <- "Random Forest"
final_comparison[2, 2] <- round(performance(p, measure="auc", fpr.stop = .2)@y.values[[1]], 3)
plot(performance(p, measure = 'tpr', x.measure = 'fpr'), main = "Random Forest")


## Regularized Logistic Regression
## Shape the train data for glmnet()
x <- model.matrix(Purchase~., data = d_train)
y <- data.matrix(d_train$Purchase)
## Shape the test data
x.test <- model.matrix(Purchase~., data = d_test)
y.test <- data.matrix(d_test$Purchase)
## Learn the model
opti_glm <- glmnet(x, y, alpha = 0.25, family = "gaussian", lambda = 0.01)
## Predict with the model
pred <- predict(opti_glm, newx = x.test)
p <- prediction(pred, d_test$Purchase)
final_comparison[3, 1] <- "Regularized Logistic Regression"
final_comparison[3, 2] <- round(performance(p, measure="auc", fpr.stop = .2)@y.values[[1]], 3)
plot(performance(p, measure = 'tpr', x.measure = 'fpr'), main = "Regularized Logistic Regression")
```

```{r, echo=F}
kableExtra::kable(final_comparison) %>% kableExtra::kable_styling(position = "center",latex_options = "HOLD_position")
```

## Setting cutoff threshold

```{r, echo=F}
## regularized logistic regression
perf.acc.log <- performance(p, measure = "acc", )
perf.tpr.log <- performance(p, measure = "tpr")
perf.fpr.log <- performance(p, measure = "fpr")

rbind(cbind(x = unlist(perf.acc.log@x.values), 
      y = unlist(perf.acc.log@y.values), metric = "acc"),
cbind(x = unlist(perf.tpr.log@x.values), 
      y = unlist(perf.tpr.log@y.values), metric = "tpr"),
cbind(x = unlist(perf.fpr.log@x.values), 
      y = unlist(perf.fpr.log@y.values), metric = "fpr")) %>% as_tibble() %>% 
  mutate(x = as.numeric(x), y = as.numeric(y)) %>% 
  ggplot(aes(x, y, color = metric)) + geom_line(size = 1)+theme_bw()+
  xlab("Cut-off")+ ylab("Accuracy / FPR / TPR")
```

The optimal cut-oof seems to be $0.15$ **1)** because we can see that this is the value where the accuracy is reaching a threshold and **2)** because this is where the difference between the True Positive Rate and the False Positive Rate is the greatest. This second remark is important because here, we are looking for maximizing the number of true positive while minimizing the number of False Positive. 

Now, we can compute the confusion matrix with a threshold of $0.15$:
```{r, echo=F}
table(pred = ifelse(pred < .15, 0, 1), obs = d_test$Purchase)
```
