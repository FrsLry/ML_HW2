--- 
title: |
  | Introduction to Machine Learning
  | (NPFL054)
author: "Fran√ßois Leroy, PhD student at CZU"
date: "`r Sys.Date()`"
output: pdf_document
documentclass: report
classoption: oneside
fontsize: 12pt
linestretch: 1.5
geometry:
- left = 2cm
- right = 2cm
- top = 2cm
- bottom = 2cm
colorlinks: yes
link-citations: yes
github-repo: FrsLry/HW1_ML_CUNI_leroy
subtitle: Homework 2
---

```{r, echo = F}
knitr::opts_chunk$set(warning = F,
                      message = F)
```

# Set up the project {-}

```{r}
rm(list = ls())
library(ISLR) # for the data
library(tidyverse) # convenient
library(rpart) # for decision trees
library(randomForest) # for ensemble learning
library(glmnet) # for regularized logistic regression
library(ROCR) # for ROC curves
```

```{r}
## Reproduce the result
set.seed(123)
## Create the splitting vector
split <- sample(nrow(Caravan), 1000)
## Create the test dataset
d_test <- Caravan[split,]
## Create the training dataset
d_train <- Caravan[-split,]
```


# Task 1 - Data analysis

* **First, check the distribution of the target attribute. What would be your precision if you select 100 examples by chance?**

```{r, echo=F, eval=F}
round(table(Caravan$Purchase), 2)
```

```{r, echo=F}
Caravan %>% 
  ggplot()+
  geom_bar(aes(x = Purchase, y = ..prop.., group = 1), stat = "count")+
  ylab("Proportion")+
  theme_bw()
```



We can see that there is `r paste0(round(table(Caravan$Purchase)/nrow(Caravan), 2)[1]*100, "%")` of customers who didn't purchase the insurance and that `r paste0(round(table(Caravan$Purchase)/nrow(Caravan), 2)[2]*100, "%")` who did. From this, we can compute the following Probability Mass Function of this binomial distribution: 

```{r}
plot(dbinom(1:20, size = 100, prob = .06))
```

The precision is the number of examples classified as *Yes* when the value is actually *Yes*. Here, the precision should be `r round(table(Caravan$Purchase)/nrow(Caravan), 2)[2]`, which is actually the ratio between "Yes" and "No".

* **1.a. Focus on the customer type MOSHOOFD: create a table with the number of customers that belong to each of 10 L2 groups and the percentage of customers that purchased a caravan insurance policy in each group. Comment the figures in the table. Then do the same for the customer subtype MOSTYPE (41 subgroups defined in L1).** 


\underline{MOSHOOFD type:}

```{r}
Caravan %>% 
  count(MOSHOOFD, Purchase) %>% 
  group_by(MOSHOOFD) %>% 
  summarise(size = sum(n),
            purchase_prop = round(n[Purchase == "Yes"]/sum(n), 2)) %>% 
  rename(group = MOSHOOFD) %>% 
  kableExtra::kable()
```

This table shows the number of individuals (column $size$) and the proportion of customers that will buy an insurance in each group (column $purchase\_prop$) of the *MOSHOOFD* variable. The *MOSHOOFD* attribute correspond to the customer main type. We can see that the customers that are more prone to purchase an insurance are the one belonging to the group 2, *i.e.* the *driven growers* (13% of them will buy an insurance). Then, the *successful hedonist* are more likely to buy an insurance (*group 1*, $9\%$ of them). The names of these two groups suggest that they are rather wealthy individuals. On the other hand, the customers belonging to the class 6 and 10, respectively the *cruising seniors* and the *farmers*, are less likely to subscribe to the insurance (only 2% in each group). This is also quite expected, as seniors and farmers can be in precarious situations.

\underline{MOSTYPE type:}

```{r}
table <- 
Caravan %>% 
  count(MOSTYPE, Purchase) %>% 
  group_by(MOSTYPE) %>% 
  summarise(size = sum(n),
            purchase_prop = round(n[Purchase == "Yes"]/sum(n), 2)) %>% 
  rename(group = MOSTYPE) %>% 
  arrange(desc(purchase_prop))
## Display in 2 columns
kableExtra::kable(list(table[1:(nrow(table)/2),], 
                       table[((nrow(table)/2)+1):nrow(table),])) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")
```

This table is the same than the previous one but for the *MOSTYPE* variable, which gives more information about the social status. It is order by descending proportion of purchase. The two groups more prone to buy an insurance are the group 8 and 12, which correspond respectively to *middle class families* and *affluent young families*. Thus, we can say that families are potential good targets to sell insurances. We can see that the class 25, 26, 27 and 29 all have a low proportion of individuals buying a insurance. They are all related to old people (*i.e.*, *Young seniors in the city*, *Own home elderly*, *Seniors in apartments*, *Porchless seniors: no front yard*). Thus, as said just above for the *MOSHOOFD* variable, old people don't seem to be good targets to sell insurances. Moreover, the group 41, *i.e.* the *mixed rurals* are also less prone to subscribe an insurance, as expected with the *MOSHOOFD* variable.  

**1.b. Analyze the relationship between features MOSHOOFD and MOSTYPE.**

```{r, echo=F}
Caravan %>% 
  ggplot(aes(y = MOSTYPE, x = MOSHOOFD))+
  geom_point()+
  geom_smooth(method = "lm")+
  theme_bw()
```

We can clearly see a relationship between these two features which are MOSHOOFD = *Customer main type* and MOSTYPE = *Customer Subtype*. This is expected because MOSTYPE is just a more precise social position. For instance, we can see that when $MOSHOOFD = 10$, $MOSTYPE = 40 | 41$. We can see that $MOSHOOFD = 10$ correspond to *Farmers* and that $MOSTYPE = 40 | 41$ are two subclasses of farmers: *Large family farms* and *Mixed rurals*, respectively. 

# Task 2 - Model fitting, optimization, and selection

```{r}
## Function to randomly extract the test dataset in d_train 
## using always the same number of positive and negative 
## values of Purchase
prepare_cv_folds <-  function(k){
  # Create the subsets data containing Purchase == Yes 
  # in one hand and Purchase == No in an other hand
  pos_data <- d_train[d_train$Purchase == "Yes",]
  neg_data <- d_train[d_train$Purchase == "No",]
  ## Compute the size of each fold
  fold.size.pos <- nrow(pos_data)%/%k
  fold.size.neg <- nrow(neg_data)%/%k
  ## Randomly rearrange the indexes
  set.seed(12); s_pos <- sample(nrow(pos_data))
  set.seed(12); s_neg <- sample(nrow(neg_data))
  ## create the list that will contain the test folds
  f.idx <-  list()
  ## For each fold, extract the dataset that will be used as test
  for(i in 1:k){
      f.idx[[i]] <- 
        rbind(pos_data[s_pos[(1 + (i-1)*fold.size.pos):(i*fold.size.pos)],],
              neg_data[s_neg[(1 + (i-1)*fold.size.neg):(i*fold.size.neg)],])
  }
  return(f.idx)
}
## Use the function to create the 10 test datasets
split_data <- prepare_cv_folds(10)
```

## Decision tree

```{r, echo=F, eval=F}
set.seed(123)
## Sequence of cp
seq <- seq(1e-05, 0.001, length.out = 10)
## table that will contain the auc for each fold
auc_table <- tibble(auc = as.numeric())
## table summarizing the cv for each cp
dt_auc <- tibble(cp = as.numeric(),
                 mean_auc = as.numeric(),
                 sd = as.numeric(),
                 low_CI = as.numeric(),
                 high_CI  = as.numeric())
## Loop over each cp
for(j in 1:length(seq)){
  ## Loop over each fold
  for(i in 1:10){
    ## Get the indexes of the test datasets
    test_index <- rownames(split_data[[i]])
    cv_train_data <-
      d_train %>% 
      filter(!rownames(.) %in% test_index) %>% 
      mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
      mutate(Purchase = as.factor(Purchase))
    cv_test_data <-
      split_data[[i]] %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
    ## Learn decision tree without the test dataset 
    DT <- rpart(Purchase ~ ., cp = seq[j],
                data = cv_train_data)
    pred <- predict(DT, (cv_test_data %>% select(-Purchase)), type = "prob")[,2]
    p <- prediction(pred, cv_test_data$Purchase)
    auc <- performance(p, measure="auc", fpr.stop = .2)
    auc <- auc@y.values[[1]]
    auc_table[i, 1] <- auc
    # plot(performance(p, measure = 'tpr', x.measure = 'fpr'))
  }
  ## Fill the table
  dt_auc[j, 1] <- seq[j]
  dt_auc[j, 2] <- mean(auc_table$auc)
  dt_auc[j, 3] <- sd(auc_table$auc)
  dt_auc[j, 4] <- t.test(auc_table$auc)$"conf.int"[1]
  dt_auc[j, 5] <- t.test(auc_table$auc)$"conf.int"[2]
}
## Plot the table
dt_auc %>% 
  ggplot(aes(x = cp, y = mean_auc))+
  geom_point()+
  geom_point(aes(y = high_CI), color = "red")+
  geom_point(aes(y = low_CI), color = "red")+
  geom_pointrange(aes(ymin = mean_auc - sd, ymax = mean_auc + sd))+
  # ylim(0, .05)+
  theme_bw() 
ggsave("data/cv_dt.png")
```



```{r, echo=F}
knitr::include_graphics("data/cv_dt.png")
```


The graphique above shows the mean AUC as a function of different values of cp. The black lines represent the standard deviation and the red dots the Confidence Intervals (computed with the `t.test()` function and with $\alpha = 5\%$). 
As we can see the mean $AUC_{0.2}$ stay stable for the six firsts values of $cp$ and increase a bit for $cp = 6.7\times10^{-3}$ and then stay stable with increasing $cp$. reducing the complexity parameter below $cp = 0.001$ doesn't change the mean $AUC_{0.2}$. The $cp$ parameter indicates the minimum change of the training error rate to consider the splitting process. As a low $cp$ means a more complex model, we are looking for the highest value of cp maximizing the mean $AUC_{0.2}$. We can see that $cp = 0.001$ is already sufficiently low. Thus, we can select $cp = 0.001$ to learn the decision tree. 


## Random Forest


```{r, echo=F, eval=F}
set.seed(123)
## Sequence of mtry
seq_mtry <- round(seq(2, 89, length.out = 6))
## Sequence of trees
seq_tree <- round(seq(10, 150, length.out = 10))
## table that will contain the auc for each fold
auc_table <- tibble(auc = as.numeric())
## table summarizing the cv for each cp
dt_auc <- tibble(ntree = as.numeric(),
                 mean_auc = as.numeric(),
                 sd = as.numeric(),
                 low_CI = as.numeric(),
                 high_CI  = as.numeric(),
                 mtry = as.numeric())
## Loop over each mtry
for(m in 1:length(seq_mtry)){
  ## Loop over each ntree
  for(j in 1:length(seq_tree)){
    ## Loop over each fold
    for(i in 1:10){
      ## Get the indexes of the test datasets
      test_index <- rownames(split_data[[i]])
      cv_train_data <-
        d_train %>% 
        filter(!rownames(.) %in% test_index) %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
      cv_test_data <-
        split_data[[i]] %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
      ## Learn decision tree without the test dataset 
      DT <- randomForest(Purchase ~ ., ntree = seq_tree[j], mtry = seq_mtry[m],
                  data = cv_train_data)
      pred <- predict(DT, (cv_test_data %>% select(-Purchase)), type = "prob")[,2]
      p <- prediction(pred, cv_test_data$Purchase)
      auc <- performance(p, measure="auc", fpr.stop = .2)
      auc <- auc@y.values[[1]]
      auc_table[i, 1] <- auc
      # plot(performance(p, measure = 'tpr', x.measure = 'fpr'))
    }
    ## Fill the table
    dt_auc[nrow(dt_auc)+1, 1] <- seq_tree[j]
    dt_auc[nrow(dt_auc), 2] <- mean(auc_table$auc)
    dt_auc[nrow(dt_auc), 3] <- sd(auc_table$auc)
    dt_auc[nrow(dt_auc), 4] <- ifelse(length(unique(auc_table$auc)) == 1,
                                      NA, t.test(auc_table$auc)$"conf.int"[1])
    dt_auc[nrow(dt_auc), 5] <- ifelse(length(unique(auc_table$auc)) == 1, 
                                      NA, t.test(auc_table$auc)$"conf.int"[2])  
    dt_auc[nrow(dt_auc), 6] <- seq_mtry[m]
  }
  
}
## Plot the table
dt_auc %>% 
  ggplot(aes(x = ntree, y = mean_auc))+
  geom_point()+
  geom_point(aes(y = high_CI), color = "red")+
  geom_point(aes(y = low_CI), color = "red")+
  facet_wrap(. ~ mtry, labeller = labeller(.cols = label_both))+
  geom_pointrange(aes(ymin = mean_auc - sd, ymax = mean_auc + sd))+
  # ylim(0, .05)+
  theme_bw()

ggsave("data/cv_rf.png")
```

```{r, echo=F}
knitr::include_graphics("data/cv_rf.png")
```


This plot shows the mean auc as a function of the number of trees (*i.e.* $ntree$). Each square correspond to a value of $mtry$, *i.e.* the number of features used in the splitting process. As we can see, the highest value of $AUC_{0.2}$ is for $mtry = 19$ and $ntree = 120$.

We know that the theoretical value of $mtry$ for the classification task is $\sqrt{number\:of\:feature}$. Here, it is equal to $10$, which is quite close to the selected $mtry = 19$. 

## Regularized logistic regression

```{r, echo=F, eval=F}
# 5 different alpha values
alphas <- seq(0, 1, length.out = 5)
# 100 different lambda values
grid <- round(10^seq(4, -2, length = 10), 2)
## table that will contain the auc for each fold
auc_table <- tibble(auc = as.numeric())
## Table that will contain the summarized info
glmnet_table <- tibble(alpha = as.numeric(),
                       lambda = as.numeric(),
                       mean_auc = as.numeric(),
                       sd = as.numeric(),
                       low_CI = as.numeric(),
                       high_CI  = as.numeric())
## Loop over each alpha
for(iii in 1:length(alphas)){
  ## Loop over each lambda
  for(ii in 1:length(grid)){
    ## Loop over each fold
    for(i in 1:10){
      ## Get the indexes of the test datasets
      test_index <- rownames(split_data[[i]])
      cv_train_data <-
        d_train %>% 
        filter(!rownames(.) %in% test_index) %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
      cv_test_data <-
        split_data[[i]] %>% 
        mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
        mutate(Purchase = as.factor(Purchase))
      ## Shape the train data for glmnet()
      x <- model.matrix(Purchase~., data = cv_train_data)
      y <- data.matrix(cv_train_data$Purchase)
      ## Shape the test data
      x.test <- model.matrix(Purchase~., data = cv_test_data)
      y.test <- data.matrix(cv_test_data$Purchase)
      ## Learn the model
      model <- glmnet(x, y, alpha = alphas[iii], family = "binomial", lambda = grid[ii])
      ## Predict with the model
      pred <- predict(model, newx = x.test, type = "response", s = grid[ii])
      p <- prediction(pred, cv_test_data$Purchase)
      ## Compute auc
      auc <- performance(p, measure="auc", fpr.stop = .2)
      auc <- auc@y.values[[1]]
      auc_table[i, 1] <- auc
      # plot(performance(p, measure = 'tpr', x.measure = 'fpr'))
    }
    glmnet_table[nrow(glmnet_table)+1, 1] <- alphas[iii]
    glmnet_table[nrow(glmnet_table), 2] <- grid[ii] 
    glmnet_table[nrow(glmnet_table), 3] <- mean(auc_table$auc)
    glmnet_table[nrow(glmnet_table), 4] <- sd(auc_table$auc)
    glmnet_table[nrow(glmnet_table), 5] <- ifelse(length(unique(auc_table$auc)) == 1,
                                                  NA, t.test(auc_table$auc)$"conf.int"[1])
    glmnet_table[nrow(glmnet_table), 6]<- ifelse(length(unique(auc_table$auc)) == 1,
                                                 NA, t.test(auc_table$auc)$"conf.int"[2])
  }
}
## Plot
glmnet_table %>% 
  ggplot(aes(x = as.factor(lambda), y = mean_auc))+
  geom_point()+
  geom_point(aes(y = high_CI), color = "red")+
  geom_point(aes(y = low_CI), color = "red")+
  facet_wrap(. ~ alpha, labeller = labeller(.cols = label_both))+
  geom_pointrange(aes(ymin = mean_auc - sd, ymax = mean_auc + sd))+
  # ylim(0, .05)+
  xlab("lambda")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
ggsave("data/cv_lr.png")
```

```{r, echo=F}
knitr::include_graphics("data/cv_lr.png")
```

This figure shows the mean $AUC_{0.2}$ for different values of two hyperparameters of the elastic net regularization: 

* $\alpha$: which is the weight given to the two types of penalties (L1 and L2, see *Element of Statistical Learning*, Hastie *et al.* 2001). $\alpha = 1$ correspond to the lasso penalty (*i.e.* the L1 penalty) and $\alpha = 0$ correspont to the ridge regularization (*i.e.* the L2 penalty).

* $\lambda$: which is the weighting of the penalties to the loss function. Thus, $\lambda = 0$ means no weighting and is an unregularized logistic regression whilst $\lambda = 1$ means a fully weighted penalty.

As we can see on this figures, the highest value of $AUC_{0.2}$ seems to be for $\alpha = 0.75$ and $\lambda = 0.01$. 

## Note about hyperparameters selection

As represented on the previous plots, most of the $AUC_{0.2}$ confidence intervals overlap, which means that their differences are statistically non significant. However, a choice was necessary and I decided to always choose the hyperparameters giving the lowest $AUC_{0.2}$ values.

## Evaluation on the test dataset

Thanks to the previous steps, we have chosen:

1. Decision tree with $cp = 0.001$

2. Random forest with $ntree = 120$ and $mtry = 19$

3. Regularized logistic regression with $\lambda = 0.01$ and $\alpha = 0.75$

Now, we can **1)** learn the model on the entire training dataset and **2)** compute the $AUC_{0.2}$ when predicting on the test dataset.

The following plots show the ROC curve (Receiver Operating Characteristic) of the three optimized models with their respective $AUC_{0.2}$ displayed in the table. 

```{r, echo=F, fig.show="hold", out.width="150%"}
par(mfrow = c(2, 2))
final_comparison <- tibble(model =as.character(),
                           auc = as.numeric())
d_train <- 
  d_train %>% 
  mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
  mutate(Purchase = as.factor(Purchase))  

d_test <- 
  d_test %>% 
  mutate(Purchase = ifelse(Purchase == "Yes", 1, 0)) %>% 
  mutate(Purchase = as.factor(Purchase))
  
## DT
opti_DT <- rpart(Purchase ~ ., data = d_train, cp = 0.001)
pred <- predict(opti_DT, d_test %>% select(-Purchase), type = "prob")[,2]
p <- prediction(pred, d_test$Purchase)
final_comparison[1, 1] <- "Decision Tree"
final_comparison[1, 2] <- round(performance(p, measure="auc", fpr.stop = .2)@y.values[[1]], 3)
plot(performance(p, measure = 'tpr', x.measure = 'fpr'), main = "Decision Tree")


## RF
opti_RF <- randomForest(Purchase ~ ., data = d_train, ntree = 120, mtry = 19)
pred <- predict(opti_RF, d_test %>% select(-Purchase), type = "prob")[,2]
p <- prediction(pred, d_test$Purchase)
final_comparison[2, 1] <- "Random Forest"
final_comparison[2, 2] <- round(performance(p, measure="auc", fpr.stop = .2)@y.values[[1]], 3)
plot(performance(p, measure = 'tpr', x.measure = 'fpr'), main = "Random Forest")


## Regularized Logistic Regression
## Shape the train data for glmnet()
x <- model.matrix(Purchase~., data = d_train)
y <- data.matrix(d_train$Purchase)
## Shape the test data
x.test <- model.matrix(Purchase~., data = d_test)
y.test <- data.matrix(d_test$Purchase)
## Learn the model
opti_glm <- glmnet(x, y, alpha = 0.75, family = "binomial", lambda = 0.01)
## Predict with the model
pred <- predict(opti_glm, newx = x.test, type = "response", s = 0.01)
p <- prediction(pred, d_test$Purchase)
final_comparison[3, 1] <- "Regularized Logistic Regression"
final_comparison[3, 2] <- round(performance(p, measure="auc", fpr.stop = .2)@y.values[[1]], 3)
plot(performance(p, measure = 'tpr', x.measure = 'fpr'), main = "Regularized Logistic Regression")
```

```{r, echo=F}
kableExtra::kable(final_comparison) %>% kableExtra::kable_styling(position = "center",latex_options = "HOLD_position")
```

As we can see, the highest value of $AUC_{0.2}$ is for the decision tree with $cp = 0.001$. It is interesting to note the difference between the $AUC_{0.2}$ computed on the test dataset and the one computed from the cross-validation. From the cross-validation, the highest value of $AUC_{0.2}$ was for the regularized logistic regression, with $AUC_{0.2}\approx0.07$ whilst for the decision tree it was slightly lower than $0.065$. Now, we can see that $AUC_{0.2}$ is higher for the decision tree. We can think that the decision tree is better at generalization than the regularized logistic regression, which was slightly overfitted.  

## Setting cutoff threshold

The cut-off indicates the probability at which we should start to consider the prediction as True. The following plot shows the values of precision, accuracy, True Positive Rate (TPR) and False Positive Rate (FPR) according to different cut-off threshold. So far, all the learning processes have been done using the $AUC_{0.2}$, which is the Area Under the Curve just up to $FPR\leq20\%$. Thus, I decided not to consider cut-off higher than 0.2. 

```{r, echo=F}
set.seed(123)
## DT
opti_DT <- rpart(Purchase ~ ., data = d_train, cp = 0.001)
pred <- predict(opti_DT, d_test %>% select(-Purchase), type = "prob")[,2]
p <- prediction(pred, d_test$Purchase)
perf.prec <- performance(p, measure = "prec")
perf.acc <- performance(p, measure = "acc")
perf.tpr <- performance(p, measure = "tpr")
perf.fpr <- performance(p, measure = "fpr")

rbind(cbind(x = unlist(perf.prec@x.values), 
      y = unlist(perf.prec@y.values), metric = "precision"),
cbind(x = unlist(perf.acc@x.values),
      y = unlist(perf.acc@y.values), metric = "accuracy"),
cbind(x = unlist(perf.tpr@x.values), 
      y = unlist(perf.tpr@y.values), metric = "tpr"),
cbind(x = unlist(perf.fpr@x.values), 
      y = unlist(perf.fpr@y.values), metric = "fpr")) %>% as_tibble() %>% 
  mutate(x = as.numeric(x), y = as.numeric(y)) %>% 
  ggplot(aes(x, y, color = metric)) + geom_line(size = 1)+theme_bw()+
  xlab("Cut-off")+ ylab("Precision / Accuracy / FPR / TPR")+xlim(0, 0.2)
```

First, we can see that the precision and accuracy start closely to what a random classifier would do, *i.e.* close to $0.06$. We can also observe that the maximum value of accuracy is very quickly reach: this is due to the fact that our dataset is really homogeneous, *i.e.* we have a lot of *No* for very little *Yes*. Thus, the classifier will quickly correctly classify all the actual *No* as *No* for low values of the cut-off and the accuracy will be high. Here, we need to focus on the correctly classified *Yes*, and for this we use the precision ($P = \frac{TP}{TP+FP}$). We can see that the highest value of precision is reached for $cutoff = 0.2$. 

Now, we can compute the confusion matrix with a threshold of $0.2$:
```{r, echo=F}
table(pred = ifelse(pred < .2, 0, 1), obs = d_test$Purchase)
```

# Task 3 - Model interpretation and feature selection


```{r, echo=F}
table3 <- opti_DT[["variable.importance"]] %>% as.data.frame() %>% rename(., MeanDecreaseGini = .) %>% rownames_to_column(var = "feature") %>% as_tibble() %>% mutate(MeanDecreaseGini = round(MeanDecreaseGini, 2))
## Display in 2 columns
kableExtra::kable(list(table3[1:(nrow(table3)/2),], 
                       table3[((nrow(table3)/2)+1):nrow(table3),]), row.names = F,
                  caption = "Variable importance for the decision tree") %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position", font_size = 9)
```

```{r, echo=F}
table2 <- importance(opti_RF) %>% as.data.frame() %>% arrange(desc(MeanDecreaseGini)) %>% rownames_to_column(var = "feature") %>% mutate(MeanDecreaseGini = round(MeanDecreaseGini, 2))
## Display in 2 columns
kableExtra::kable(list(table2[1:round((nrow(table2)/3)),], 
                       table2[round(((nrow(table2)/3)+1)):floor(2*nrow(table2)/3),],
                       table2[floor(((2*nrow(table2)/3)+1)):round(nrow(table2)),]),
                  row.names = F, caption = "Variable importance for the random forest") %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position", font_size = 9)
```


```{r, echo=F}
set.seed(123)
# 100 different lambda values
grid <- 10^seq(-1, -8, length = 10)
## Learn the lasso regression
lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial", lambda = grid, type.measure = "mse")
## Use lambda from the cross validation
optimized_lasso <- glmnet(x, y, alpha =  1, family = "binomial", lambda = lasso$lambda.min)
table4 <-
  as.data.frame(as.matrix(coef(optimized_lasso))) %>%
  rownames_to_column(var = "feature") %>%
  as_tibble() %>% arrange(desc(abs(s0)))
table4 <- table4[2:32,]
## Display in 2 column
kableExtra::kable(list(table4[1:(nrow(table4)/2),], 
                       table4[floor((nrow(table4)/2)+1):nrow(table4),]), row.names = F,
                  caption = "Variable importance for the lasso regression") %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position", font_size = 9)
```

# Task 4 - Final prediction on the blind test set

The final prediction is done using the Decision Tree with a complexity parameter of $0.00067$ chosen because of its highest AUC.

```{r, echo=F}
set.seed(123)
T <- read.delim("C:/Users/leroy/Desktop/leroy_francois_hw2/data/test_data_t.csv", header=FALSE)
colnames(T) <- colnames(d_train)[-86]
## DT
final_pred <- predict(opti_DT, T, type = "prob")[,2]
final_pred <- final_pred %>% as_tibble() %>% mutate(id = seq(nrow(.))) %>% arrange(desc(value))
final_pred[1:100, 1] <- 1
final_pred[101:nrow(final_pred), 1] <- 0
final_pred <- final_pred %>% arrange(id) %>% pull(value)
write(final_pred, "data/T.prediction.txt", ncolumns = 1)
```

